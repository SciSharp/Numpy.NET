// Copyright (c) 2020 by Meinrad Recheis (Member of SciSharp)
// Code generated by CodeMinion: https://github.com/SciSharp/CodeMinion

using System;
using System.Collections;
using System.Collections.Generic;
using System.IO;
using System.Linq;
using System.Runtime.InteropServices;
using System.Text;
using Python.Runtime;
using Numpy.Models;
#if PYTHON_INCLUDED
using Python.Included;
#endif

namespace Numpy
{
    public static partial class np
    {
        
        /// <summary>
        ///	Dot product of two arrays.<br></br>
        ///	 Specifically,
        /// </summary>
        /// <param name="a">
        ///	First argument.
        /// </param>
        /// <param name="b">
        ///	Second argument.
        /// </param>
        /// <param name="out">
        ///	Output argument.<br></br>
        ///	This must have the exact kind that would be returned
        ///	if it was not used.<br></br>
        ///	In particular, it must have the right type, must be
        ///	C-contiguous, and its dtype must be the dtype that would be returned
        ///	for dot(a,b).<br></br>
        ///	This is a performance feature.<br></br>
        ///	Therefore, if these
        ///	conditions are not met, an exception is raised, instead of attempting
        ///	to be flexible.
        /// </param>
        /// <returns>
        ///	Returns the dot product of a and b.<br></br>
        ///	  If a and b are both
        ///	scalars or both 1-D arrays then a scalar is returned; otherwise
        ///	an array is returned.<br></br>
        ///	
        ///	If out is given, then it is returned.
        /// </returns>
        public static NDarray dot(NDarray a, NDarray b, NDarray @out = null)
        {
            //auto-generated code, do not change
            var __self__=self;
            var pyargs=ToTuple(new object[]
            {
                a,
                b,
            });
            var kwargs=new PyDict();
            if (@out!=null) kwargs["out"]=ToPython(@out);
            dynamic py = __self__.InvokeMethod("dot", pyargs, kwargs);
            return ToCsharp<NDarray>(py);
        }
        
        public static partial class linalg {
            /// <summary>
            ///	Compute the dot product of two or more arrays in a single function call,
            ///	while automatically selecting the fastest evaluation order.<br></br>
            ///	
            ///	multi_dot chains numpy.dot and uses optimal parenthesization
            ///	of the matrices [1] [2].<br></br>
            ///	 Depending on the shapes of the matrices,
            ///	this can speed up the multiplication a lot.<br></br>
            ///	
            ///	If the first argument is 1-D it is treated as a row vector.<br></br>
            ///	
            ///	If the last argument is 1-D it is treated as a column vector.<br></br>
            ///	
            ///	The other arguments must be 2-D.<br></br>
            ///	
            ///	Think of multi_dot as:
            ///	
            ///	Notes
            ///	
            ///	The cost for a matrix multiplication can be calculated with the
            ///	following function:
            ///	
            ///	Let’s assume we have three matrices
            ///	.
            ///	
            ///	The costs for the two different parenthesizations are as follows:
            ///	
            ///	References
            /// </summary>
            /// <param name="arrays">
            ///	If the first argument is 1-D it is treated as row vector.<br></br>
            ///	
            ///	If the last argument is 1-D it is treated as column vector.<br></br>
            ///	
            ///	The other arguments must be 2-D.
            /// </param>
            /// <returns>
            ///	Returns the dot product of the supplied arrays.
            /// </returns>
            public static NDarray multi_dot(params NDarray[] arrays)
            {
                //auto-generated code, do not change
                var linalg = self.GetAttr("linalg");
                var __self__=linalg;
                var pyargs=ToTuple(new object[]
                {
                    arrays,
                });
                var kwargs=new PyDict();
                dynamic py = __self__.InvokeMethod("multi_dot", pyargs, kwargs);
                return ToCsharp<NDarray>(py);
            }
        }
        
        /// <summary>
        ///	Return the dot product of two vectors.<br></br>
        ///	
        ///	The vdot(a, b) function handles complex numbers differently than
        ///	dot(a, b).<br></br>
        ///	  If the first argument is complex the complex conjugate
        ///	of the first argument is used for the calculation of the dot product.<br></br>
        ///	
        ///	Note that vdot handles multidimensional arrays differently than dot:
        ///	it does not perform a matrix product, but flattens input arguments
        ///	to 1-D vectors first.<br></br>
        ///	 Consequently, it should only be used for vectors.
        /// </summary>
        /// <param name="a">
        ///	If a is complex the complex conjugate is taken before calculation
        ///	of the dot product.
        /// </param>
        /// <param name="b">
        ///	Second argument to the dot product.
        /// </param>
        /// <returns>
        ///	Dot product of a and b.<br></br>
        ///	  Can be an int, float, or
        ///	complex depending on the types of a and b.
        /// </returns>
        public static NDarray vdot(NDarray a, NDarray b)
        {
            //auto-generated code, do not change
            var __self__=self;
            var pyargs=ToTuple(new object[]
            {
                a,
                b,
            });
            var kwargs=new PyDict();
            dynamic py = __self__.InvokeMethod("vdot", pyargs, kwargs);
            return ToCsharp<NDarray>(py);
        }
        
        /// <summary>
        ///	Inner product of two arrays.<br></br>
        ///	
        ///	Ordinary inner product of vectors for 1-D arrays (without complex
        ///	conjugation), in higher dimensions a sum product over the last axes.<br></br>
        ///	
        ///	Notes
        ///	
        ///	For vectors (1-D arrays) it computes the ordinary inner-product:
        ///	
        ///	More generally, if ndim(a) = r &gt; 0 and ndim(b) = s &gt; 0:
        ///	
        ///	or explicitly:
        ///	
        ///	In addition a or b may be scalars, in which case:
        /// </summary>
        /// <param name="b">
        ///	If a and b are nonscalar, their last dimensions must match.
        /// </param>
        /// <param name="a">
        ///	If a and b are nonscalar, their last dimensions must match.
        /// </param>
        /// <returns>
        ///	out.shape = a.shape[:-1] + b.shape[:-1]
        /// </returns>
        public static NDarray inner(NDarray b, NDarray a)
        {
            //auto-generated code, do not change
            var __self__=self;
            var pyargs=ToTuple(new object[]
            {
                b,
                a,
            });
            var kwargs=new PyDict();
            dynamic py = __self__.InvokeMethod("inner", pyargs, kwargs);
            return ToCsharp<NDarray>(py);
        }
        
        /// <summary>
        ///	Compute the outer product of two vectors.<br></br>
        ///	
        ///	Given two vectors, a = [a0, a1, ..., aM] and
        ///	b = [b0, b1, ..., bN],
        ///	the outer product [1] is:
        ///	
        ///	References
        /// </summary>
        /// <param name="a">
        ///	First input vector.<br></br>
        ///	Input is flattened if
        ///	not already 1-dimensional.
        /// </param>
        /// <param name="b">
        ///	Second input vector.<br></br>
        ///	Input is flattened if
        ///	not already 1-dimensional.
        /// </param>
        /// <param name="out">
        ///	A location where the result is stored
        /// </param>
        /// <returns>
        ///	out[i, j] = a[i] * b[j]
        /// </returns>
        public static NDarray outer(NDarray a, NDarray b, NDarray @out = null)
        {
            //auto-generated code, do not change
            var __self__=self;
            var pyargs=ToTuple(new object[]
            {
                a,
                b,
            });
            var kwargs=new PyDict();
            if (@out!=null) kwargs["out"]=ToPython(@out);
            dynamic py = __self__.InvokeMethod("outer", pyargs, kwargs);
            return ToCsharp<NDarray>(py);
        }
        
        /// <summary>
        ///	Matrix product of two arrays.<br></br>
        ///	
        ///	Notes
        ///	
        ///	The behavior depends on the arguments in the following way.<br></br>
        ///	
        ///	matmul differs from dot in two important ways:
        ///	
        ///	The matmul function implements the semantics of the &#64; operator introduced
        ///	in Python 3.5 following PEP465.
        /// </summary>
        /// <param name="x2">
        ///	Input arrays, scalars not allowed.
        /// </param>
        /// <param name="x1">
        ///	Input arrays, scalars not allowed.
        /// </param>
        /// <param name="out">
        ///	A location into which the result is stored.<br></br>
        ///	If provided, it must have
        ///	a shape that matches the signature (n,k),(k,m)-&gt;(n,m).<br></br>
        ///	If not
        ///	provided or None, a freshly-allocated array is returned.
        /// </param>
        /// <returns>
        ///	The matrix product of the inputs.<br></br>
        ///	
        ///	This is a scalar only when both x1, x2 are 1-d vectors.
        /// </returns>
        public static NDarray matmul(NDarray x2, NDarray x1, NDarray @out = null)
        {
            //auto-generated code, do not change
            var __self__=self;
            var pyargs=ToTuple(new object[]
            {
                x2,
                x1,
            });
            var kwargs=new PyDict();
            if (@out!=null) kwargs["out"]=ToPython(@out);
            dynamic py = __self__.InvokeMethod("matmul", pyargs, kwargs);
            return ToCsharp<NDarray>(py);
        }
        
        /// <summary>
        ///	Compute tensor dot product along specified axes for arrays &gt;= 1-D.<br></br>
        ///	
        ///	Given two tensors (arrays of dimension greater than or equal to one),
        ///	a and b, and an array_like object containing two array_like
        ///	objects, (a_axes, b_axes), sum the products of a’s and b’s
        ///	elements (components) over the axes specified by a_axes and
        ///	b_axes.<br></br>
        ///	 The third argument can be a single non-negative
        ///	integer_like scalar, N; if it is such, then the last N
        ///	dimensions of a and the first N dimensions of b are summed
        ///	over.<br></br>
        ///	
        ///	Notes
        ///	
        ///	When axes is integer_like, the sequence for evaluation will be: first
        ///	the -Nth axis in a and 0th axis in b, and the -1th axis in a and
        ///	Nth axis in b last.<br></br>
        ///	
        ///	When there is more than one axis to sum over - and they are not the last
        ///	(first) axes of a (b) - the argument axes should consist of
        ///	two sequences of the same length, with the first axis to sum over given
        ///	first in both sequences, the second axis second, and so forth.
        /// </summary>
        /// <param name="b">
        ///	Tensors to “dot”.
        /// </param>
        /// <param name="a">
        ///	Tensors to “dot”.
        /// </param>
        public static NDarray tensordot(NDarray b, NDarray a, int[] axes = null)
        {
            //auto-generated code, do not change
            var __self__=self;
            var pyargs=ToTuple(new object[]
            {
                b,
                a,
            });
            var kwargs=new PyDict();
            if (axes!=null) kwargs["axes"]=ToPython(axes);
            dynamic py = __self__.InvokeMethod("tensordot", pyargs, kwargs);
            return ToCsharp<NDarray>(py);
        }
        
        /// <summary>
        ///	Evaluates the Einstein summation convention on the operands.<br></br>
        ///	
        ///	Using the Einstein summation convention, many common multi-dimensional,
        ///	linear algebraic array operations can be represented in a simple fashion.<br></br>
        ///	
        ///	In implicit mode einsum computes these values.<br></br>
        ///	
        ///	In explicit mode, einsum provides further flexibility to compute
        ///	other array operations that might not be considered classical Einstein
        ///	summation operations, by disabling, or forcing summation over specified
        ///	subscript labels.<br></br>
        ///	
        ///	See the notes and examples for clarification.<br></br>
        ///	
        ///	Notes
        ///	
        ///	The Einstein summation convention can be used to compute
        ///	many multi-dimensional, linear algebraic array operations.<br></br>
        ///	 einsum
        ///	provides a succinct way of representing these.<br></br>
        ///	
        ///	A non-exhaustive list of these operations,
        ///	which can be computed by einsum, is shown below along with examples:
        ///	
        ///	The subscripts string is a comma-separated list of subscript labels,
        ///	where each label refers to a dimension of the corresponding operand.<br></br>
        ///	
        ///	Whenever a label is repeated it is summed, so np.einsum('i,i', a, b)
        ///	is equivalent to np.inner(a,b).<br></br>
        ///	 If a label
        ///	appears only once, it is not summed, so np.einsum('i', a) produces a
        ///	view of a with no changes.<br></br>
        ///	 A further example np.einsum('ij,jk', a, b)
        ///	describes traditional matrix multiplication and is equivalent to
        ///	np.matmul(a,b).<br></br>
        ///	 Repeated subscript labels in one
        ///	operand take the diagonal.<br></br>
        ///	 For example, np.einsum('ii', a) is equivalent
        ///	to np.trace(a).<br></br>
        ///	
        ///	In implicit mode, the chosen subscripts are important
        ///	since the axes of the output are reordered alphabetically.<br></br>
        ///	  This
        ///	means that np.einsum('ij', a) doesn’t affect a 2D array, while
        ///	np.einsum('ji', a) takes its transpose.<br></br>
        ///	 Additionally,
        ///	np.einsum('ij,jk', a, b) returns a matrix multiplication, while,
        ///	np.einsum('ij,jh', a, b) returns the transpose of the
        ///	multiplication since subscript ‘h’ precedes subscript ‘i’.
        ///	
        ///	In explicit mode the output can be directly controlled by
        ///	specifying output subscript labels.<br></br>
        ///	  This requires the
        ///	identifier ‘-&gt;’ as well as the list of output subscript labels.<br></br>
        ///	
        ///	This feature increases the flexibility of the function since
        ///	summing can be disabled or forced when required.<br></br>
        ///	 The call
        ///	np.einsum('i-&gt;', a) is like np.sum(a, axis=-1),
        ///	and np.einsum('ii-&gt;i', a) is like np.diag(a).<br></br>
        ///	
        ///	The difference is that einsum does not allow broadcasting by default.<br></br>
        ///	
        ///	Additionally np.einsum('ij,jh-&gt;ih', a, b) directly specifies the
        ///	order of the output subscript labels and therefore returns matrix
        ///	multiplication, unlike the example above in implicit mode.<br></br>
        ///	
        ///	To enable and control broadcasting, use an ellipsis.<br></br>
        ///	  Default
        ///	NumPy-style broadcasting is done by adding an ellipsis
        ///	to the left of each term, like np.einsum('...ii-&gt;...i', a).<br></br>
        ///	
        ///	To take the trace along the first and last axes,
        ///	you can do np.einsum('i...i', a), or to do a matrix-matrix
        ///	product with the left-most indices instead of rightmost, one can do
        ///	np.einsum('ij...,jk...-&gt;ik...', a, b).<br></br>
        ///	
        ///	When there is only one operand, no axes are summed, and no output
        ///	parameter is provided, a view into the operand is returned instead
        ///	of a new array.<br></br>
        ///	  Thus, taking the diagonal as np.einsum('ii-&gt;i', a)
        ///	produces a view (changed in version 1.10.0).<br></br>
        ///	
        ///	einsum also provides an alternative way to provide the subscripts
        ///	and operands as einsum(op0, sublist0, op1, sublist1, ..., [sublistout]).<br></br>
        ///	
        ///	If the output shape is not provided in this format einsum will be
        ///	calculated in implicit mode, otherwise it will be performed explicitly.<br></br>
        ///	
        ///	The examples below have corresponding einsum calls with the two
        ///	parameter methods.<br></br>
        ///	
        ///	Views returned from einsum are now writeable whenever the input array
        ///	is writeable.<br></br>
        ///	 For example, np.einsum('ijk...-&gt;kji...', a) will now
        ///	have the same effect as np.swapaxes(a, 0, 2)
        ///	and np.einsum('ii-&gt;i', a) will return a writeable view of the diagonal
        ///	of a 2D array.<br></br>
        ///	
        ///	Added the optimize argument which will optimize the contraction order
        ///	of an einsum expression.<br></br>
        ///	 For a contraction with three or more operands this
        ///	can greatly increase the computational efficiency at the cost of a larger
        ///	memory footprint during computation.<br></br>
        ///	
        ///	Typically a ‘greedy’ algorithm is applied which empirical tests have shown
        ///	returns the optimal path in the majority of cases.<br></br>
        ///	 In some cases ‘optimal’
        ///	will return the superlative path through a more expensive, exhaustive search.<br></br>
        ///	
        ///	For iterative calculations it may be advisable to calculate the optimal path
        ///	once and reuse that path by supplying it as an argument.<br></br>
        ///	 An example is given
        ///	below.<br></br>
        ///	
        ///	See numpy.einsum_path for more details.
        /// </summary>
        /// <param name="subscripts">
        ///	Specifies the subscripts for summation as comma separated list of
        ///	subscript labels.<br></br>
        ///	An implicit (classical Einstein summation)
        ///	calculation is performed unless the explicit indicator ‘-&gt;’ is
        ///	included as well as subscript labels of the precise output form.
        /// </param>
        /// <param name="operands">
        ///	These are the arrays for the operation.
        /// </param>
        /// <param name="out">
        ///	If provided, the calculation is done into this array.
        /// </param>
        /// <param name="dtype">
        ///	If provided, forces the calculation to use the data type specified.<br></br>
        ///	
        ///	Note that you may have to also give a more liberal casting
        ///	parameter to allow the conversions.<br></br>
        ///	Default is None.
        /// </param>
        /// <param name="order">
        ///	Controls the memory layout of the output.<br></br>
        ///	‘C’ means it should
        ///	be C contiguous.<br></br>
        ///	‘F’ means it should be Fortran contiguous,
        ///	‘A’ means it should be ‘F’ if the inputs are all ‘F’, ‘C’ otherwise.<br></br>
        ///	
        ///	‘K’ means it should be as close to the layout as the inputs as
        ///	is possible, including arbitrarily permuted axes.<br></br>
        ///	
        ///	Default is ‘K’.
        /// </param>
        /// <param name="casting">
        ///	Controls what kind of data casting may occur.<br></br>
        ///	Setting this to
        ///	‘unsafe’ is not recommended, as it can adversely affect accumulations.<br></br>
        ///	
        ///	Default is ‘safe’.
        /// </param>
        /// <param name="optimize">
        ///	Controls if intermediate optimization should occur.<br></br>
        ///	No optimization
        ///	will occur if False and True will default to the ‘greedy’ algorithm.<br></br>
        ///	
        ///	Also accepts an explicit contraction list from the np.einsum_path
        ///	function.<br></br>
        ///	See np.einsum_path for more details.<br></br>
        ///	Defaults to False.
        /// </param>
        /// <returns>
        ///	The calculation based on the Einstein summation convention.
        /// </returns>
        public static NDarray einsum(string subscripts, NDarray[] operands, NDarray @out = null, Dtype dtype = null, string order = null, string casting = "safe", object optimize = null)
        {
            //auto-generated code, do not change
            var __self__=self;
            var pyargs=ToTuple(new object[]
            {
                subscripts
            }
            .Concat(operands)
            .ToArray());
            var kwargs=new PyDict();
            if (@out!=null) kwargs["out"]=ToPython(@out);
            if (dtype!=null) kwargs["dtype"]=ToPython(dtype);
            if (order!=null) kwargs["order"]=ToPython(order);
            if (casting!="safe") kwargs["casting"]=ToPython(casting);
            if (optimize!=null) kwargs["optimize"]=ToPython(optimize);
            dynamic py = __self__.InvokeMethod("einsum", pyargs, kwargs);
            return ToCsharp<NDarray>(py);
        }
        
        /*
        /// <summary>
        ///	Evaluates the lowest cost contraction order for an einsum expression by
        ///	considering the creation of intermediate arrays.<br></br>
        ///	
        ///	Notes
        ///	
        ///	The resulting path indicates which terms of the input contraction should be
        ///	contracted first, the result of this contraction is then appended to the
        ///	end of the contraction list.<br></br>
        ///	 This list can then be iterated over until all
        ///	intermediate contractions are complete.
        /// </summary>
        /// <param name="subscripts">
        ///	Specifies the subscripts for summation.
        /// </param>
        /// <param name="operands">
        ///	These are the arrays for the operation.
        /// </param>
        /// <param name="optimize">
        ///	Choose the type of path.<br></br>
        ///	If a tuple is provided, the second argument is
        ///	assumed to be the maximum intermediate size created.<br></br>
        ///	If only a single
        ///	argument is provided the largest input or output array size is used
        ///	as a maximum intermediate size.<br></br>
        ///	
        ///	Default is ‘greedy’.
        /// </param>
        /// <returns>
        /// A tuple of:
        /// path
        ///	A list representation of the einsum path.
        /// string_repr
        ///	A printable representation of the einsum path.
        /// </returns>
        public static (list of tuples, string) einsum_path(string subscripts, NDarray[] operands, string optimize = "greedy")
        {
            //auto-generated code, do not change
            var __self__=self;
            var pyargs=ToTuple(new object[]
            {
                subscripts,
                operands,
            });
            var kwargs=new PyDict();
            if (optimize!="greedy") kwargs["optimize"]=ToPython(optimize);
            dynamic py = __self__.InvokeMethod("einsum_path", pyargs, kwargs);
            return (ToCsharp<list of tuples>(py[0]), ToCsharp<string>(py[1]));
        }
        */
        
        public static partial class linalg {
            /// <summary>
            ///	Raise a square matrix to the (integer) power n.<br></br>
            ///	
            ///	For positive integers n, the power is computed by repeated matrix
            ///	squarings and matrix multiplications.<br></br>
            ///	 If n == 0, the identity matrix
            ///	of the same shape as M is returned.<br></br>
            ///	 If n &lt; 0, the inverse
            ///	is computed and then raised to the abs(n).
            /// </summary>
            /// <param name="a">
            ///	Matrix to be “powered.”
            /// </param>
            /// <param name="n">
            ///	The exponent can be any integer or long integer, positive,
            ///	negative, or zero.
            /// </param>
            /// <returns>
            ///	The return value is the same shape and type as M;
            ///	if the exponent is positive or zero then the type of the
            ///	elements is the same as those of M.<br></br>
            ///	 If the exponent is
            ///	negative the elements are floating-point.
            /// </returns>
            public static NDarray matrix_power(NDarray a, int n)
            {
                //auto-generated code, do not change
                var linalg = self.GetAttr("linalg");
                var __self__=linalg;
                var pyargs=ToTuple(new object[]
                {
                    a,
                    n,
                });
                var kwargs=new PyDict();
                dynamic py = __self__.InvokeMethod("matrix_power", pyargs, kwargs);
                return ToCsharp<NDarray>(py);
            }
        }
        
        /// <summary>
        ///	Kronecker product of two arrays.<br></br>
        ///	
        ///	Computes the Kronecker product, a composite array made of blocks of the
        ///	second array scaled by the first.<br></br>
        ///	
        ///	Notes
        ///	
        ///	The function assumes that the number of dimensions of a and b
        ///	are the same, if necessary prepending the smallest with ones.<br></br>
        ///	
        ///	If a.shape = (r0,r1,..,rN) and b.shape = (s0,s1,…,sN),
        ///	the Kronecker product has shape (r0*s0, r1*s1, …, rN*SN).<br></br>
        ///	
        ///	The elements are products of elements from a and b, organized
        ///	explicitly by:
        ///	
        ///	where:
        ///	
        ///	In the common 2-D case (N=1), the block structure can be visualized:
        /// </summary>
        public static NDarray kron(NDarray b, NDarray a)
        {
            //auto-generated code, do not change
            var __self__=self;
            var pyargs=ToTuple(new object[]
            {
                b,
                a,
            });
            var kwargs=new PyDict();
            dynamic py = __self__.InvokeMethod("kron", pyargs, kwargs);
            return ToCsharp<NDarray>(py);
        }
        
        public static partial class linalg {
            /// <summary>
            ///	Compute the qr factorization of a matrix.<br></br>
            ///	
            ///	Factor the matrix a as qr, where q is orthonormal and r is
            ///	upper-triangular.<br></br>
            ///	
            ///	Notes
            ///	
            ///	This is an interface to the LAPACK routines dgeqrf, zgeqrf,
            ///	dorgqr, and zungqr.<br></br>
            ///	
            ///	For more information on the qr factorization, see for example:
            ///	https://en.wikipedia.org/wiki/QR_factorization
            ///	
            ///	Subclasses of ndarray are preserved except for the ‘raw’ mode.<br></br>
            ///	 So if
            ///	a is of type matrix, all the return values will be matrices too.<br></br>
            ///	
            ///	New ‘reduced’, ‘complete’, and ‘raw’ options for mode were added in
            ///	NumPy 1.8.0 and the old option ‘full’ was made an alias of ‘reduced’.  In
            ///	addition the options ‘full’ and ‘economic’ were deprecated.<br></br>
            ///	  Because
            ///	‘full’ was the previous default and ‘reduced’ is the new default,
            ///	backward compatibility can be maintained by letting mode default.<br></br>
            ///	
            ///	The ‘raw’ option was added so that LAPACK routines that can multiply
            ///	arrays by q using the Householder reflectors can be used.<br></br>
            ///	 Note that in
            ///	this case the returned arrays are of type np.double or np.cdouble and
            ///	the h array is transposed to be FORTRAN compatible.<br></br>
            ///	  No routines using
            ///	the ‘raw’ return are currently exposed by numpy, but some are available
            ///	in lapack_lite and just await the necessary work.
            /// </summary>
            /// <param name="a">
            ///	Matrix to be factored.
            /// </param>
            /// <param name="mode">
            ///	If K = min(M, N), then
            ///	
            ///	The options ‘reduced’, ‘complete, and ‘raw’ are new in numpy 1.8,
            ///	see the notes for more information.<br></br>
            ///	The default is ‘reduced’, and to
            ///	maintain backward compatibility with earlier versions of numpy both
            ///	it and the old default ‘full’ can be omitted.<br></br>
            ///	Note that array h
            ///	returned in ‘raw’ mode is transposed for calling Fortran.<br></br>
            ///	The
            ///	‘economic’ mode is deprecated.<br></br>
            ///	The modes ‘full’ and ‘economic’ may
            ///	be passed using only the first letter for backwards compatibility,
            ///	but all others must be spelled out.<br></br>
            ///	See the Notes for more
            ///	explanation.
            /// </param>
            /// <returns>
            /// A tuple of:
            /// q
            ///	A matrix with orthonormal columns. When mode = ‘complete’ the
            ///	result is an orthogonal/unitary matrix depending on whether or not
            ///	a is real/complex. The determinant may be either +/- 1 in that
            ///	case.
            /// r
            ///	The upper-triangular matrix.
            /// (h, tau)
            ///	The array h contains the Householder reflectors that generate q
            ///	along with r. The tau array contains scaling factors for the
            ///	reflectors. In the deprecated  ‘economic’ mode only h is returned.
            /// </returns>
            public static (NDarray, NDarray, NDarray) qr(NDarray a, string mode = "reduced")
            {
                //auto-generated code, do not change
                var linalg = self.GetAttr("linalg");
                var __self__=linalg;
                var pyargs=ToTuple(new object[]
                {
                    a,
                });
                var kwargs=new PyDict();
                if (mode!="reduced") kwargs["mode"]=ToPython(mode);
                dynamic py = __self__.InvokeMethod("qr", pyargs, kwargs);
                return (ToCsharp<NDarray>(py[0]), ToCsharp<NDarray>(py[1]), ToCsharp<NDarray>(py[2]));
            }
        }
        
        /*
        public static partial class linalg {
            /// <summary>
            ///	Compute the condition number of a matrix.<br></br>
            ///	
            ///	This function is capable of returning the condition number using
            ///	one of seven different norms, depending on the value of p (see
            ///	Parameters below).<br></br>
            ///	
            ///	Notes
            ///	
            ///	The condition number of x is defined as the norm of x times the
            ///	norm of the inverse of x [1]; the norm can be the usual L2-norm
            ///	(root-of-sum-of-squares) or one of a number of other matrix norms.<br></br>
            ///	
            ///	References
            /// </summary>
            /// <param name="x">
            ///	The matrix whose condition number is sought.
            /// </param>
            /// <param name="p">
            ///	Order of the norm:
            ///	
            ///	inf means the numpy.inf object, and the Frobenius norm is
            ///	the root-of-sum-of-squares norm.
            /// </param>
            /// <returns>
            ///	The condition number of the matrix.<br></br>
            ///	 May be infinite.
            /// </returns>
            public static {float cond(NDarray x, {None p = null)
            {
                //auto-generated code, do not change
                var linalg = self.GetAttr("linalg");
                var __self__=linalg;
                var pyargs=ToTuple(new object[]
                {
                    x,
                });
                var kwargs=new PyDict();
                if (p!=null) kwargs["p"]=ToPython(p);
                dynamic py = __self__.InvokeMethod("cond", pyargs, kwargs);
                return ToCsharp<{float>(py);
            }
        }
        */
        
        public static partial class linalg {
            /// <summary>
            ///	Return matrix rank of array using SVD method
            ///	
            ///	Rank of the array is the number of singular values of the array that are
            ///	greater than tol.<br></br>
            ///	
            ///	Notes
            ///	
            ///	The default threshold to detect rank deficiency is a test on the magnitude
            ///	of the singular values of M.<br></br>
            ///	  By default, we identify singular values less
            ///	than S.max() * max(M.shape) * eps as indicating rank deficiency (with
            ///	the symbols defined above).<br></br>
            ///	 This is the algorithm MATLAB uses [1].<br></br>
            ///	  It also
            ///	appears in Numerical recipes in the discussion of SVD solutions for linear
            ///	least squares [2].<br></br>
            ///	
            ///	This default threshold is designed to detect rank deficiency accounting for
            ///	the numerical errors of the SVD computation.<br></br>
            ///	  Imagine that there is a column
            ///	in M that is an exact (in floating point) linear combination of other
            ///	columns in M.<br></br>
            ///	 Computing the SVD on M will not produce a singular value
            ///	exactly equal to 0 in general: any difference of the smallest SVD value from
            ///	0 will be caused by numerical imprecision in the calculation of the SVD.<br></br>
            ///	
            ///	Our threshold for small SVD values takes this numerical imprecision into
            ///	account, and the default threshold will detect such numerical rank
            ///	deficiency.<br></br>
            ///	  The threshold may declare a matrix M rank deficient even if
            ///	the linear combination of some columns of M is not exactly equal to
            ///	another column of M but only numerically very close to another column of
            ///	M.<br></br>
            ///	
            ///	We chose our default threshold because it is in wide use.<br></br>
            ///	  Other thresholds
            ///	are possible.<br></br>
            ///	  For example, elsewhere in the 2007 edition of Numerical
            ///	recipes there is an alternative threshold of S.max() *
            ///	np.finfo(M.dtype).eps / 2.<br></br>
            ///	 * np.sqrt(m + n + 1.).<br></br>
            ///	 The authors describe
            ///	this threshold as being based on “expected roundoff error” (p 71).<br></br>
            ///	
            ///	The thresholds above deal with floating point roundoff error in the
            ///	calculation of the SVD.<br></br>
            ///	  However, you may have more information about the
            ///	sources of error in M that would make you consider other tolerance values
            ///	to detect effective rank deficiency.<br></br>
            ///	  The most useful measure of the
            ///	tolerance depends on the operations you intend to use on your matrix.<br></br>
            ///	  For
            ///	example, if your data come from uncertain measurements with uncertainties
            ///	greater than floating point epsilon, choosing a tolerance near that
            ///	uncertainty may be preferable.<br></br>
            ///	  The tolerance may be absolute if the
            ///	uncertainties are absolute rather than relative.<br></br>
            ///	
            ///	References
            /// </summary>
            /// <param name="M">
            ///	input vector or stack of matrices
            /// </param>
            /// <param name="tol">
            ///	threshold below which SVD values are considered zero.<br></br>
            ///	If tol is
            ///	None, and S is an array with singular values for M, and
            ///	eps is the epsilon value for datatype of S, then tol is
            ///	set to S.max() * max(M.shape) * eps.
            /// </param>
            /// <param name="hermitian">
            ///	If True, M is assumed to be Hermitian (symmetric if real-valued),
            ///	enabling a more efficient method for finding singular values.<br></br>
            ///	
            ///	Defaults to False.
            /// </param>
            public static int matrix_rank(NDarray M, NDarray tol = null, bool? hermitian = false)
            {
                //auto-generated code, do not change
                var linalg = self.GetAttr("linalg");
                var __self__=linalg;
                var pyargs=ToTuple(new object[]
                {
                    M,
                });
                var kwargs=new PyDict();
                if (tol!=null) kwargs["tol"]=ToPython(tol);
                if (hermitian!=false) kwargs["hermitian"]=ToPython(hermitian);
                dynamic py = __self__.InvokeMethod("matrix_rank", pyargs, kwargs);
                return ToCsharp<int>(py);
            }
        }
        
        public static partial class linalg {
            /// <summary>
            ///	Compute the sign and (natural) logarithm of the determinant of an array.<br></br>
            ///	
            ///	If an array has a very small or very large determinant, then a call to
            ///	det may overflow or underflow.<br></br>
            ///	 This routine is more robust against such
            ///	issues, because it computes the logarithm of the determinant rather than
            ///	the determinant itself.<br></br>
            ///	
            ///	Notes
            ///	
            ///	Broadcasting rules apply, see the numpy.linalg documentation for
            ///	details.<br></br>
            ///	
            ///	The determinant is computed via LU factorization using the LAPACK
            ///	routine z/dgetrf.
            /// </summary>
            /// <param name="a">
            ///	Input array, has to be a square 2-D array.
            /// </param>
            /// <returns>
            /// A tuple of:
            /// sign
            ///	A number representing the sign of the determinant. For a real matrix,
            ///	this is 1, 0, or -1. For a complex matrix, this is a complex number
            ///	with absolute value 1 (i.e., it is on the unit circle), or else 0.
            /// logdet
            ///	The natural log of the absolute value of the determinant.
            /// </returns>
            public static (NDarray, NDarray) slogdet(NDarray a)
            {
                //auto-generated code, do not change
                var linalg = self.GetAttr("linalg");
                var __self__=linalg;
                var pyargs=ToTuple(new object[]
                {
                    a,
                });
                var kwargs=new PyDict();
                dynamic py = __self__.InvokeMethod("slogdet", pyargs, kwargs);
                return (ToCsharp<NDarray>(py[0]), ToCsharp<NDarray>(py[1]));
            }
        }
        
        /// <summary>
        ///	Return the sum along diagonals of the array.<br></br>
        ///	
        ///	If a is 2-D, the sum along its diagonal with the given offset
        ///	is returned, i.e., the sum of elements a[i,i+offset] for all i.<br></br>
        ///	
        ///	If a has more than two dimensions, then the axes specified by axis1 and
        ///	axis2 are used to determine the 2-D sub-arrays whose traces are returned.<br></br>
        ///	
        ///	The shape of the resulting array is the same as that of a with axis1
        ///	and axis2 removed.
        /// </summary>
        /// <param name="a">
        ///	Input array, from which the diagonals are taken.
        /// </param>
        /// <param name="offset">
        ///	Offset of the diagonal from the main diagonal.<br></br>
        ///	Can be both positive
        ///	and negative.<br></br>
        ///	Defaults to 0.
        /// </param>
        /// <param name="axis2">
        ///	Axes to be used as the first and second axis of the 2-D sub-arrays
        ///	from which the diagonals should be taken.<br></br>
        ///	Defaults are the first two
        ///	axes of a.
        /// </param>
        /// <param name="axis1">
        ///	Axes to be used as the first and second axis of the 2-D sub-arrays
        ///	from which the diagonals should be taken.<br></br>
        ///	Defaults are the first two
        ///	axes of a.
        /// </param>
        /// <param name="dtype">
        ///	Determines the data-type of the returned array and of the accumulator
        ///	where the elements are summed.<br></br>
        ///	If dtype has the value None and a is
        ///	of integer type of precision less than the default integer
        ///	precision, then the default integer precision is used.<br></br>
        ///	Otherwise,
        ///	the precision is the same as that of a.
        /// </param>
        /// <param name="out">
        ///	Array into which the output is placed.<br></br>
        ///	Its type is preserved and
        ///	it must be of the right shape to hold the output.
        /// </param>
        /// <returns>
        ///	If a is 2-D, the sum along the diagonal is returned.<br></br>
        ///	  If a has
        ///	larger dimensions, then an array of sums along diagonals is returned.
        /// </returns>
        public static NDarray trace(NDarray a, int? offset = 0, int? axis2 = null, int? axis1 = null, Dtype dtype = null, NDarray @out = null)
        {
            //auto-generated code, do not change
            var __self__=self;
            var pyargs=ToTuple(new object[]
            {
                a,
            });
            var kwargs=new PyDict();
            if (offset!=0) kwargs["offset"]=ToPython(offset);
            if (axis2!=null) kwargs["axis2"]=ToPython(axis2);
            if (axis1!=null) kwargs["axis1"]=ToPython(axis1);
            if (dtype!=null) kwargs["dtype"]=ToPython(dtype);
            if (@out!=null) kwargs["out"]=ToPython(@out);
            dynamic py = __self__.InvokeMethod("trace", pyargs, kwargs);
            return ToCsharp<NDarray>(py);
        }
        
        public static partial class linalg {
            /// <summary>
            ///	Solve the tensor equation a x = b for x.<br></br>
            ///	
            ///	It is assumed that all indices of x are summed over in the product,
            ///	together with the rightmost indices of a, as is done in, for example,
            ///	tensordot(a, x, axes=b.ndim).
            /// </summary>
            /// <param name="a">
            ///	Coefficient tensor, of shape b.shape + Q.<br></br>
            ///	Q, a tuple, equals
            ///	the shape of that sub-tensor of a consisting of the appropriate
            ///	number of its rightmost indices, and must be such that
            ///	prod(Q) == prod(b.shape) (in which sense a is said to be
            ///	‘square’).
            /// </param>
            /// <param name="b">
            ///	Right-hand tensor, which can be of any shape.
            /// </param>
            /// <param name="axes">
            ///	Axes in a to reorder to the right, before inversion.<br></br>
            ///	
            ///	If None (default), no reordering is done.
            /// </param>
            public static NDarray tensorsolve(NDarray a, NDarray b, int[] axes = null)
            {
                //auto-generated code, do not change
                var linalg = self.GetAttr("linalg");
                var __self__=linalg;
                var pyargs=ToTuple(new object[]
                {
                    a,
                    b,
                });
                var kwargs=new PyDict();
                if (axes!=null) kwargs["axes"]=ToPython(axes);
                dynamic py = __self__.InvokeMethod("tensorsolve", pyargs, kwargs);
                return ToCsharp<NDarray>(py);
            }
        }
        
        public static partial class linalg {
            /// <summary>
            ///	Compute the ‘inverse’ of an N-dimensional array.<br></br>
            ///	
            ///	The result is an inverse for a relative to the tensordot operation
            ///	tensordot(a, b, ind), i.<br></br>
            ///	 e., up to floating-point accuracy,
            ///	tensordot(tensorinv(a), a, ind) is the “identity” tensor for the
            ///	tensordot operation.
            /// </summary>
            /// <param name="a">
            ///	Tensor to ‘invert’. Its shape must be ‘square’, i.<br></br>
            ///	e.,
            ///	prod(a.shape[:ind]) == prod(a.shape[ind:]).
            /// </param>
            /// <param name="ind">
            ///	Number of first indices that are involved in the inverse sum.<br></br>
            ///	
            ///	Must be a positive integer, default is 2.
            /// </param>
            /// <returns>
            ///	a’s tensordot inverse, shape a.shape[ind:] + a.shape[:ind].
            /// </returns>
            public static NDarray tensorinv(NDarray a, int? ind = 2)
            {
                //auto-generated code, do not change
                var linalg = self.GetAttr("linalg");
                var __self__=linalg;
                var pyargs=ToTuple(new object[]
                {
                    a,
                });
                var kwargs=new PyDict();
                if (ind!=2) kwargs["ind"]=ToPython(ind);
                dynamic py = __self__.InvokeMethod("tensorinv", pyargs, kwargs);
                return ToCsharp<NDarray>(py);
            }
        }
        
        public static partial class linalg {
            /// <summary>
            ///	Generic Python-exception-derived object raised by linalg functions.<br></br>
            ///	
            ///	General purpose exception class, derived from Python’s exception.Exception
            ///	class, programmatically raised in linalg functions when a Linear
            ///	Algebra-related condition would prevent further correct execution of the
            ///	function.
            /// </summary>
            public static void LinAlgError()
            {
                //auto-generated code, do not change
                var linalg = self.GetAttr("linalg");
                var __self__=linalg;
                dynamic py = __self__.InvokeMethod("LinAlgError");
            }
        }
        
        
    }
}
